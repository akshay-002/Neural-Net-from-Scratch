{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bb481-795e-4262-a24c-41c6d2f9877d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\">PARAMETER INITIALIZATION</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f051f3bb-7eb5-4e07-ba7d-a1e83e870a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:09:12.890096Z",
     "start_time": "2024-05-10T09:09:12.879874Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes the parameters of the deep neural network with zeros.\n",
    "\n",
    "    Arguments:\n",
    "    layers_dims -- list containing the dimensions of each layer in the network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized parameters 'W1', 'b1', ..., 'WL', 'bL'\n",
    "                   where Wl is weight matrix of shape (layers_dims[l], layers_dims[l-1])\n",
    "                   and bl is bias vector of shape (layers_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f1363754df77879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:09:47.054245Z",
     "start_time": "2024-05-10T09:09:47.037729Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes the parameters of the deep neural network with random values.\n",
    "\n",
    "    Arguments:\n",
    "    layers_dims -- list containing the dimensions of each layer in the network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized parameters 'W1', 'b1', ..., 'WL', 'bL'\n",
    "                   where Wl is weight matrix of shape (layers_dims[l], layers_dims[l-1])\n",
    "                   and bl is bias vector of shape (layers_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)  # Set random seed for reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)  # Number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a220134c31849f16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T09:10:10.773143Z",
     "start_time": "2024-05-10T09:10:10.761396Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes the parameters of the deep neural network using He initialization.\n",
    "\n",
    "    Arguments:\n",
    "    layers_dims -- list containing the dimensions of each layer in the network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized parameters 'W1', 'b1', ..., 'WL', 'bL'\n",
    "                   where Wl is weight matrix of shape (layers_dims[l], layers_dims[l-1])\n",
    "                   and bl is bias vector of shape (layers_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)  # Set random seed for reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1  # Number of layers in the network\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9393b87-4393-42ba-b90a-64d51abfa9ff",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\">ACTIVATION FUNCTION</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a26ab5-0a07-4050-9920-d03bd224d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the ReLU activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation output, same shape as Z\n",
    "    cache -- a python dictionary containing \"Z\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z  # We store Z since it's used in backpropagation\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c34a4b-e032-4f14-970c-e921b823a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implement the sigmoid activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation output, same shape as Z\n",
    "    cache -- a python dictionary containing \"Z\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0203ce9-f22d-4e63-8e17-df2bbe3fb3df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\">FORWARD PROPAGATION</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786199b9-dae1-41b0-ab79-c82c177fc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a tuple containing \"A\", \"W\" and \"b\"; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b488325-5a66-40ad-a107-ce8bcc3785b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function (post-activation value)\n",
    "    cache -- a tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf25739-945f-447f-95d9-4d17b08af8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34735dd0-f42c-4aee-b057-dbaa68549e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    Implements forward propagation with dropout for L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "                  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                  bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    keep_prob -- probability of keeping a neuron active during dropout, scalar\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    caches = []\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    A = X\n",
    "    # Loop through the layers\n",
    "    for l in range(1, L):\n",
    "        # Forward propagation for hidden layers\n",
    "        Z = np.dot(parameters['W' + str(l)], A) + parameters['b' + str(l)]\n",
    "        A = relu(Z)\n",
    "\n",
    "        # Dropout\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = A * D\n",
    "        A = A / keep_prob\n",
    "\n",
    "        # Cache the values for backward propagation\n",
    "        cache = (Z, D)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Final layer with sigmoid activation\n",
    "    Z = np.dot(parameters['W' + str(L)], A) + parameters['b' + str(L)]\n",
    "    AL = sigmoid(Z)\n",
    "\n",
    "    # Cache the values for backward propagation\n",
    "    cache = (Z, AL)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118d747-11af-4085-983c-f73a783b0d73",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> COMPUTE COST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87cf2786-91a2-4a9c-8d12-da1cc5f4144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Compute the cross-entropy cost.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to the label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost=(-1/m) * np.dot(Y,np.log(AL).T) + np.dot((1-Y),np.log(1-AL).T)\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb5f81b-8403-401e-9d50-29a7772bdbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Compute the cost of the model with L2 regularization.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- post-activation, output of forward propagation, shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    lambd -- regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    cost -- value of the regularized loss function\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    cross_entropy_cost = compute_cost(AL, Y)\n",
    "\n",
    "    L2_regularization_cost = 0\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        L2_regularization_cost += np.sum(np.square(W))\n",
    "\n",
    "    L2_regularization_cost *= lambd / (2 * m)\n",
    "\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63576b3a-1854-4fc2-881a-f251d5b03cc9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> BACKWARD  PROPAGATION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4ae161-af43-4999-84c1-2afb541f19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, same shape as A\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # Convert dZ to a correct object.\n",
    "    dZ[Z <= 0] = 0  # When Z <= 0, set dZ to 0 as well.\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67088060-dabd-4e7e-8ca3-2903ebc8650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, same shape as A\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * (s * (1 - s))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367e1fcd-848b-4ab6-9cbd-0f614ecd23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW=(1/m)* np.dot(dZ,A_prev.T)\n",
    "    db=(1/m)* np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475bddbe-a6d1-451e-a2c1-36d60ce191df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ =relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0593e263-7c53-441d-8e6f-b51fda53cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1] # Last Layer (L-1) because caches is a list\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,activation='sigmoid')\n",
    "    \n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f09c29cb-315e-4999-9197-a869bc51c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for dropout.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    cache -- dictionary containing \"Z1\", \"A1\", ..., \"ZL\", \"AL\" and \"W1\", \"b1\", ..., \"WL\", \"bL\", \"D1\", ..., \"DL\"\n",
    "    keep_prob -- probability of keeping a neuron active during dropout, scalar\n",
    "\n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients\n",
    "                 gradients[\"dA\" + str(l)] = ... \n",
    "                 gradients[\"dW\" + str(l)] = ...\n",
    "                 gradients[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    gradients = {}\n",
    "    L = len(cache) // 3  # Number of layers\n",
    "    \n",
    "    # Retrieve cache\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        dA_prev = gradients.get('dA' + str(l), None)\n",
    "        if dA_prev is None:\n",
    "            dA_prev = Y - cache['A' + str(L)]\n",
    "        \n",
    "        Z = cache['Z' + str(l)]\n",
    "        D = cache['D' + str(l)]\n",
    "        A = cache['A' + str(l)]\n",
    "        W = cache['W' + str(l)]\n",
    "        b = cache['b' + str(l)]\n",
    "        \n",
    "        # Dropout backward\n",
    "        dA = dA_prev * D / keep_prob\n",
    "        \n",
    "        # Linear activation backward\n",
    "        dZ = dA * relu_backward(Z)\n",
    "        gradients['dW' + str(l)] = 1./m * np.dot(dZ, cache['A' + str(l - 1)].T)\n",
    "        gradients['db' + str(l)] = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        gradients['dA' + str(l - 1)] = np.dot(W.T, dZ)\n",
    "        \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb5784b-ec28-47e9-aeee-79db9a80fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements backward propagation with L2 regularization.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    cache -- dictionary containing \"Z1\", \"A1\", ..., \"ZL\", \"AL\" and \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "\n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients\n",
    "                 gradients[\"dA\" + str(l)] = ... \n",
    "                 gradients[\"dW\" + str(l)] = ...\n",
    "                 gradients[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    gradients = {}\n",
    "    L = len(cache) // 3  # Number of layers\n",
    "    \n",
    "    # Retrieve cache\n",
    "    A_prev = X\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        Z = cache['Z' + str(l)]\n",
    "        A = cache['A' + str(l)]\n",
    "        W = cache['W' + str(l)]\n",
    "        b = cache['b' + str(l)]\n",
    "        \n",
    "        # Compute dZ\n",
    "        if l == L:\n",
    "            dZ = A - Y\n",
    "        else:\n",
    "            dZ = np.dot(W.T, gradients['dZ' + str(l + 1)]) * (A > 0) # ReLU derivative\n",
    "        \n",
    "        # Add regularization term to gradients\n",
    "        gradients['dW' + str(l)] = 1./m * np.dot(dZ, A_prev.T) + (lambd/m)*W\n",
    "        gradients['db' + str(l)] = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        gradients['dA' + str(l - 1)] = np.dot(W.T, dZ)\n",
    "        \n",
    "        A_prev = cache['A' + str(l - 1)]\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb059be-ef08-4a9b-a72f-6616cf061ae8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> OPTIMIZER INITIALIZATION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "651d77c0-15ba-4472-8a5f-f6eddebe32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initialize the velocity for gradient descent optimization.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters (weights and biases) of the model\n",
    "\n",
    "    Returns:\n",
    "    v -- dictionary containing the velocity for each parameter\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers in the neural networks\n",
    "    v = {}\n",
    "\n",
    "    # Initialize velocity\n",
    "    for l in range(1, L + 1):\n",
    "        v['dW' + str(l)] = np.zeros(parameters['W' + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(parameters['b' + str(l)].shape)\n",
    "\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb21a18c-a2f9-4a0b-b195-8183a2b39305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initialize the Adam optimization algorithm.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters (weights and biases) of the model\n",
    "\n",
    "    Returns:\n",
    "    v -- dictionary containing the exponentially weighted average of the gradients\n",
    "    s -- dictionary containing the exponentially weighted average of the squared gradients\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    # Initialize v, s.\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "        s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "\n",
    "    return v, s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e02279-955a-45ca-a1e4-bae098aeac13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\">PARAMETER UPDATE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08fc96c0-4fab-4c59-abf0-edb6d34106f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L+1):\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "166717d5-241e-462e-a642-a6ddee48d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum optimization algorithm.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters (weights and biases) of the model\n",
    "    grads -- dictionary containing the gradients of the cost function with respect to the parameters\n",
    "    v -- dictionary containing the exponentially weighted average of the gradients\n",
    "    beta -- the momentum hyperparameter\n",
    "    learning_rate -- the learning rate\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated parameters\n",
    "    v -- updated velocities\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers in the neural networks\n",
    "\n",
    "    # Momentum update for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        # Compute velocities\n",
    "        v[\"dW\" + str(l)] = (beta * v[\"dW\" + str(l)]) + ((1 - beta) * grads['dW' + str(l)])\n",
    "        v[\"db\" + str(l)] = (beta * v[\"db\" + str(l)]) + ((1 - beta) * grads[\"db\" + str(l)])\n",
    "\n",
    "        # Update parameters\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v[\"db\" + str(l)]\n",
    "\n",
    "    return parameters, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f0da417-ef38-47c0-986a-2fbc9248f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using the Adam optimization algorithm.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters (weights and biases) of the model\n",
    "    grads -- dictionary containing the gradients of the cost function with respect to the parameters\n",
    "    v -- exponentially weighted average of the gradients (first moment estimate)\n",
    "    s -- exponentially weighted average of the squared gradients (second moment estimate)\n",
    "    t -- Adam counter (to keep track of bias corrections)\n",
    "    learning_rate -- the learning rate\n",
    "    beta1 -- exponential decay rate for the first moment estimates\n",
    "    beta2 -- exponential decay rate for the second moment estimates\n",
    "    epsilon -- small value to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated parameters\n",
    "    v -- updated first moment estimates\n",
    "    s -- updated second moment estimates\n",
    "    v_corrected -- bias-corrected first moment estimates\n",
    "    s_corrected -- bias-corrected second moment estimates\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers in the neural networks\n",
    "    v_corrected = {}  # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}  # Initializing second moment estimate, python dictionary\n",
    "\n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients with momentum.\n",
    "        v[\"dW\" + str(l)] = (beta1 * v[\"dW\" + str(l)]) + ((1 - beta1) * grads['dW' + str(l)])\n",
    "        v[\"db\" + str(l)] = (beta1 * v[\"db\" + str(l)]) + ((1 - beta1) * grads['db' + str(l)])\n",
    "\n",
    "        # Compute bias-corrected first moment estimate.\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - beta1 ** t)\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - beta1 ** t)\n",
    "\n",
    "        # Moving average of the root mean squared gradients.\n",
    "        s[\"dW\" + str(l)] = (beta2 * s[\"dW\" + str(l)]) + ((1 - beta2) * (np.square(grads['dW' + str(l)])))\n",
    "        s[\"db\" + str(l)] = (beta2 * s[\"db\" + str(l)]) + ((1 - beta2) * (np.square(grads['db' + str(l)])))\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate.\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / (\n",
    "                    np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / (\n",
    "                    np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecef71e-b593-469e-b8a8-b57b49bada7f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> MINI BATCH</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02d922-19c1-4af0-bec5-20b2bdc524fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    \n",
    "    Creates a list of random mini-batches from the input data.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, numpy array of shape (output size, number of examples)\n",
    "    mini_batch_size -- size of each mini-batch\n",
    "    seed -- random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of mini-batches, each mini-batch is a tuple (mini_batch_X, mini_batch_Y)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X= shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y= shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa07f4b-5e7a-4d26-a5e7-c51b18fbd830",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> LEARNING RATE DECAY FUNCTIONS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbdd1b18-0119-420b-8475-55991b7bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    \"\"\"\n",
    "    Update the learning rate using exponential decay.\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate0 -- initial learning rate\n",
    "    epoch_num -- current epoch number\n",
    "    decay_rate -- rate of decay\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- updated learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # Exponential decay formula for updating learning rate\n",
    "    learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate0\n",
    "\n",
    "    return learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fcf006e-4e74-4b45-b6fe-a2a56ac2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    \"\"\"\n",
    "    Update the learning rate using exponential decay with periodic scheduling.\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate0 -- initial learning rate\n",
    "    epoch_num -- current epoch number\n",
    "    decay_rate -- rate of decay\n",
    "    time_interval -- interval for decay scheduling (default is 1000)\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- updated learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # Exponential decay with periodic scheduling\n",
    "    learning_rate = (1 / (1 + decay_rate * np.floor(epoch_num / time_interval))) * learning_rate0\n",
    "\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617f735-4aa7-4c44-b484-cc2b8ee6f9c1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\"> BUILDING NEURAL NETWORK MODEL</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3bcd52-a58c-4b6f-a017-6dd9d7993a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(X, Y, learning_rate=0.01, num_iterations=15000, print_cost=True, initialization=\"he\"):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: Linear -> Relu -> Linear -> Relu -> Linear -> Sigmoid.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, numpy array of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 1000 iterations\n",
    "    initialization -- flag to choose which initialization to use (\"zeros\", \"random\", \"he\")\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    costs = []  # to keep track of the loss\n",
    "    m = X.shape[1]  # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "\n",
    "    # Initialization of parameters\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        al, cache = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(al, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = L_model_backward(X, Y, cache)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f46d27-bb82-4f62-a35f-17967de3fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_dropout_and_regularization(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, initialization=\"he\", lambd = 0, keep_prob = 1):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Implements a neural network model with dropout and regularization.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, numpy array of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- True to print the cost every 10000 iterations\n",
    "    initialization -- type of weight initialization: \"zeros\", \"random\", or \"he\"\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob -- probability of keeping a neuron active during dropout, scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model, which can be used for prediction\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    costs = []                            \n",
    "    m = X.shape[1]                       \n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "\n",
    "    # Initialization of parameters\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        if keep_prob == 1:\n",
    "            al, cache = L_model_forward(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            al, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "\n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(al, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(al, Y, parameters, lambd)\n",
    "\n",
    "        # Backward propagation.\n",
    "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n",
    "        # but in this code block we will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = L_model_backward(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9343b5b8-11d4-42ba-8e96-87bdda4ac1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_optimizer(X, Y, layers_dims, optimizer=\"adam\", learning_rate = 0.0007,initialization=\"he\", mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5001, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Implements a neural network model with different optimization algorithms.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, numpy array of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, numpy array of shape (output size, number of examples)\n",
    "    layers_dims -- list containing the size of each layer (including input layer)\n",
    "    optimizer -- optimization algorithm to be used: \"gd\", \"momentum\", or \"adam\"\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    initialization -- type of weight initialization: \"zeros\", \"random\", or \"he\"\n",
    "    mini_batch_size -- size of mini-batches for mini-batch gradient descent\n",
    "    beta -- momentum hyperparameter\n",
    "    beta1 -- exponential decay hyperparameter for the past gradients estimates in Adam\n",
    "    beta2 -- exponential decay hyperparameter for the past squared gradients estimates in Adam\n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob -- probability of keeping a neuron active during dropout, scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "\n",
    "    # Initialize parameters\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            if keep_prob == 1:\n",
    "                al, cache = L_model_forward(X, parameters)\n",
    "            elif keep_prob < 1:\n",
    "                al, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "                \n",
    "\n",
    "            # Cost function\n",
    "            if lambd == 0:\n",
    "                cost_total += compute_cost(al, minibatch_Y)\n",
    "            else:\n",
    "                cost_total += compute_cost_with_regularization(al, minibatch_Y, parameters, lambd)\n",
    "\n",
    "\n",
    "            # Backward propagation\n",
    "\n",
    "            if lambd == 0 and keep_prob == 1:\n",
    "                grads = L_model_backward(minibatch_X, minibatch_Y, cache)\n",
    "            elif lambd != 0:\n",
    "                grads = backward_propagation_with_regularization(minibatch_X, minibatch_Y, cache, lambd)\n",
    "            elif keep_prob < 1:\n",
    "                grads = backward_propagation_with_dropout(minibatch_X, minibatch_Y, cache, keep_prob)\n",
    "\n",
    "            \n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                                     t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b82f99d8-e8af-4605-a6c8-133a5c933ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_learningrate_decay(X, Y, layers_dims, optimizer, learning_rate = 0.0007, initialization=\"he\", mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1,lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate   # the original learning rate\n",
    "\n",
    "    # Initialize parameters\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            if keep_prob == 1:\n",
    "                al, cache = L_model_forward(X, parameters)\n",
    "            elif keep_prob < 1:\n",
    "                al, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "                \n",
    "\n",
    "            # Cost function\n",
    "            if lambd == 0:\n",
    "                cost_total += compute_cost(al, minibatch_Y)\n",
    "            else:\n",
    "                cost_total += compute_cost_with_regularization(al, minibatch_Y, parameters, lambd)\n",
    "\n",
    "\n",
    "            # Backward propagation\n",
    "\n",
    "            if lambd == 0 and keep_prob == 1:\n",
    "                grads = L_model_backward(minibatch_X, minibatch_Y, cache)\n",
    "            elif lambd != 0:\n",
    "                grads = backward_propagation_with_regularization(minibatch_X, minibatch_Y, cache, lambd)\n",
    "            elif keep_prob < 1:\n",
    "                grads = backward_propagation_with_dropout(minibatch_X, minibatch_Y, cache, keep_prob)\n",
    "\n",
    "            \n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                                     t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        if decay=='update_lr': \n",
    "            learning_rate = update_lr(learning_rate0, i, decay_rate)\n",
    "        elif decay=='schedule_lr_decay':\n",
    "            learning_rate = schedule_lr_decay(learning_rate0, i, decay_rate)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
